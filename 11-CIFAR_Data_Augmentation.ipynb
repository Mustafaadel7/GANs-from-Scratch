{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1czVdIlqnImH"
      },
      "source": [
        "# Data Augmentation.\n",
        "\n",
        "In this notebook you're going to build a generator that can be used to help create data to train a classifier. There are many cases where this might be useful. If you are interested in any of these topics, you are welcome to explore the linked papers and articles!\n",
        "\n",
        "-   With smaller datasets, GANs can provide useful data augmentation that substantially [improve classifier performance](https://arxiv.org/abs/1711.04340).\n",
        "-   You have one type of data already labeled and would like to make predictions on [another related dataset for which you have no labels](https://www.nature.com/articles/s41598-019-52737-x). (You'll learn about the techniques for this use case in future notebooks!)\n",
        "-   You want to protect the privacy of the people who provided their information so you can provide access to a [generator instead of real data](https://www.ahajournals.org/doi/full/10.1161/CIRCOUTCOMES.118.005122).\n",
        "-   You have [input data with many missing values](https://arxiv.org/abs/1806.02920), where the input dimensions are correlated and you would like to train a model on complete inputs.\n",
        "-   You would like to be able to identify a real-world abnormal feature in an image [for the purpose of diagnosis](https://link.springer.com/chapter/10.1007/978-3-030-00946-5_11), but have limited access to real examples of the condition.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wU8DDM6l9rZb"
      },
      "source": [
        "## Getting Started\n",
        "\n",
        "### Data Augmentation\n",
        "Before you implement GAN-based data augmentation, you should know a bit about data augmentation in general, specifically for image datasets. It is [very common practice](https://arxiv.org/abs/1712.04621) to augment image-based datasets in ways that are appropriate for a given dataset. This may include having your dataloader randomly flipping images across their vertical axis, randomly cropping your image to a particular size, randomly adding a bit of noise or color to an image in ways that are true-to-life.\n",
        "\n",
        "In general, data augmentation helps to stop your model from overfitting to the data, and allows you to make small datasets many times larger. However, a sufficiently powerful classifier often still overfits to the original examples which is why GANs are particularly useful here. They can generate new images instead of simply modifying existing ones.\n",
        "\n",
        "### CIFAR\n",
        "The [CIFAR-10 and CIFAR-100](https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf) datasets are extremely widely used within machine learning -- they contain many thousands of “tiny” 32x32 color images of different classes representing relatively common real-world objects like airplanes and dogs, with 10 classes in CIFAR-10 and 100 classes in CIFAR-100. In CIFAR-100, there are 20 “superclasses” which each contain five classes. For example, the “fish” superclass contains “aquarium fish, flatfish, ray, shark, trout”. For the purposes of this assignment, you’ll be looking at a small subset of these images to simulate a small data regime, with only 40 images of each class for training.\n",
        "\n",
        "### Initializations\n",
        "You will begin by importing some useful libraries and packages and defining a visualization function that has been provided. You will also be re-using your conditional generator and functions code from earlier assignments. This will let you control what class of images to augment for your classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JfkorNJrnmNO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import nn\n",
        "from tqdm.auto import tqdm\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import make_grid\n",
        "from torch.utils.data import DataLoader\n",
        "torch.manual_seed(0) # Set for our testing purposes, please do not change!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def show_tensor_images(image_tensor, num_images=25, size=(3, 32, 32), nrow=5, show=True):\n",
        "    image_tensor = (image_tensor + 1) / 2\n",
        "    image_unflat = image_tensor.detach().cpu()\n",
        "    image_grid = make_grid(image_unflat[:num_images], nrow=nrow)\n",
        "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
        "    if show:\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "OVto-VJ6MST9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1A1M6kpnfxw"
      },
      "source": [
        "#### Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvO7h0LYnEJZ"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, input_dim=10, im_chan=3, hidden_dim=64):\n",
        "        super(Generator, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        # Build the neural network\n",
        "        self.gen = nn.Sequential(\n",
        "            self.make_gen_block(input_dim, hidden_dim * 4, kernel_size=4),\n",
        "            self.make_gen_block(hidden_dim * 4, hidden_dim * 2, kernel_size=4, stride=1),\n",
        "            self.make_gen_block(hidden_dim * 2, hidden_dim, kernel_size=4),\n",
        "            self.make_gen_block(hidden_dim, im_chan, kernel_size=2, final_layer=True),\n",
        "        )\n",
        "\n",
        "    def make_gen_block(self, input_channels, output_channels, kernel_size=3, stride=2, final_layer=False):\n",
        "        if not final_layer:\n",
        "            return nn.Sequential(\n",
        "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n",
        "                nn.BatchNorm2d(output_channels),\n",
        "                nn.ReLU(inplace=True),\n",
        "            )\n",
        "        else:\n",
        "            return nn.Sequential(\n",
        "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n",
        "                nn.Tanh(),\n",
        "            )\n",
        "\n",
        "    def forward(self, noise):\n",
        "        x = noise.view(len(noise), self.input_dim, 1, 1)\n",
        "        return self.gen(x)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_noise(n_samples, input_dim, device='cpu'):\n",
        "    return torch.randn(n_samples, input_dim, device=device)"
      ],
      "metadata": {
        "id": "Y6gyA4Pug9ot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def combine_vectors(x, y):\n",
        "    return torch.cat([x, y], 1)"
      ],
      "metadata": {
        "id": "w6ratqJUhAg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_one_hot_labels(labels, n_classes):\n",
        "    return F.one_hot(labels, n_classes)"
      ],
      "metadata": {
        "id": "4sKfRnmHhCvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRk_8azSq3tF"
      },
      "source": [
        "## Training\n",
        "Now you can begin training your models.\n",
        "First, you will define some new parameters:\n",
        "\n",
        "*   cifar100_shape: the number of pixels in each CIFAR image, which has dimensions 32 x 32 and three channel (for red, green, and blue) so 3 x 32 x 32\n",
        "*   n_classes: the number of classes in CIFAR100 (e.g. airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UpfJifVcmMhJ"
      },
      "outputs": [],
      "source": [
        "cifar100_shape = (3, 32, 32)\n",
        "n_classes = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJM9afuu0IuD"
      },
      "source": [
        "And you also include the same parameters from previous assignments:\n",
        "\n",
        "  *   criterion: the loss function\n",
        "  *   n_epochs: the number of times you iterate through the entire dataset when training\n",
        "  *   z_dim: the dimension of the noise vector\n",
        "  *   display_step: how often to display/visualize the images\n",
        "  *   batch_size: the number of images per forward/backward pass\n",
        "  *   lr: the learning rate\n",
        "  *   device: the device type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJlx2W71lUCv"
      },
      "outputs": [],
      "source": [
        "n_epochs = 10000\n",
        "z_dim = 64\n",
        "display_step = 500\n",
        "batch_size = 64\n",
        "lr = 0.0002\n",
        "# device = 'cuda'\n",
        "device = 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jltxAMd00TRE"
      },
      "source": [
        "Then, you want to set your generator's input dimension. Recall that for conditional GANs, the generator's input is the noise vector concatenated with the class vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tuSOzzpwlXl7"
      },
      "outputs": [],
      "source": [
        "generator_input_dim = z_dim + n_classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccQZRSYFXsHh"
      },
      "source": [
        "#### Classifier\n",
        "\n",
        "For the classifier, you will use the same code that you wrote in an earlier assignment (the same as previous code for the discriminator as well since the discriminator is a real/fake classifier)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVPxAjGSfYlX"
      },
      "outputs": [],
      "source": [
        "class Classifier(nn.Module):\n",
        "    def __init__(self, im_chan, n_classes, hidden_dim=32):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.disc = nn.Sequential(\n",
        "            self.make_classifier_block(im_chan, hidden_dim),\n",
        "            self.make_classifier_block(hidden_dim, hidden_dim * 2),\n",
        "            self.make_classifier_block(hidden_dim * 2, hidden_dim * 4),\n",
        "            self.make_classifier_block(hidden_dim * 4, n_classes, final_layer=True),\n",
        "        )\n",
        "\n",
        "    def make_classifier_block(self, input_channels, output_channels, kernel_size=3, stride=2, final_layer=False):\n",
        "        if not final_layer:\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n",
        "                nn.BatchNorm2d(output_channels),\n",
        "                nn.LeakyReLU(0.2, inplace=True),\n",
        "            )\n",
        "        else:\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n",
        "            )\n",
        "\n",
        "    def forward(self, image):\n",
        "        class_pred = self.disc(image)\n",
        "        return class_pred.view(len(class_pred), -1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYXJTxM9pzZK"
      },
      "source": [
        "#### Pre-training (Optional)\n",
        "\n",
        "You are provided the code to pre-train the models (GAN and classifier) given to you in this assignment. However, this is intended only for your personal curiosity -- for the assignment to run as intended, you should not use any checkpoints besides the ones given to you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXptQZcwrBrq"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, im_chan=3, hidden_dim=64):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.disc = nn.Sequential(\n",
        "            self.make_disc_block(im_chan, hidden_dim, stride=1),\n",
        "            self.make_disc_block(hidden_dim, hidden_dim * 2),\n",
        "            self.make_disc_block(hidden_dim * 2, hidden_dim * 4),\n",
        "            self.make_disc_block(hidden_dim * 4, 1, final_layer=True),\n",
        "        )\n",
        "\n",
        "    def make_disc_block(self, input_channels, output_channels, kernel_size=4, stride=2, final_layer=False):\n",
        "        if not final_layer:\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n",
        "                nn.BatchNorm2d(output_channels),\n",
        "                nn.LeakyReLU(0.2, inplace=True),\n",
        "            )\n",
        "        else:\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n",
        "            )\n",
        "\n",
        "    def forward(self, image):\n",
        "        disc_pred = self.disc(image)\n",
        "        return disc_pred.view(len(disc_pred), -1)\n",
        "\n",
        "def train_generator():\n",
        "    gen = Generator(generator_input_dim).to(device)\n",
        "    gen_opt = torch.optim.Adam(gen.parameters(), lr=lr)\n",
        "    discriminator_input_dim = cifar100_shape[0] + n_classes\n",
        "    disc = Discriminator(discriminator_input_dim).to(device)\n",
        "    disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)\n",
        "\n",
        "    def weights_init(m):\n",
        "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "            torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
        "        if isinstance(m, nn.BatchNorm2d):\n",
        "            torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
        "            torch.nn.init.constant_(m.bias, 0)\n",
        "    gen = gen.apply(weights_init)\n",
        "    disc = disc.apply(weights_init)\n",
        "\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    cur_step = 0\n",
        "    mean_generator_loss = 0\n",
        "    mean_discriminator_loss = 0\n",
        "    for epoch in range(n_epochs):\n",
        "        # Dataloader returns the batches and the labels\n",
        "        for real, labels in dataloader:\n",
        "            cur_batch_size = len(real)\n",
        "            # Flatten the batch of real images from the dataset\n",
        "            real = real.to(device)\n",
        "\n",
        "            # Convert the labels from the dataloader into one-hot versions of those labels\n",
        "            one_hot_labels = get_one_hot_labels(labels.to(device), n_classes).float()\n",
        "\n",
        "            image_one_hot_labels = one_hot_labels[:, :, None, None]\n",
        "            image_one_hot_labels = image_one_hot_labels.repeat(1, 1, cifar100_shape[1], cifar100_shape[2])\n",
        "\n",
        "            ### Update discriminator ###\n",
        "            # Zero out the discriminator gradients\n",
        "            disc_opt.zero_grad()\n",
        "            # Get noise corresponding to the current batch_size\n",
        "            fake_noise = get_noise(cur_batch_size, z_dim, device=device)\n",
        "\n",
        "            # Combine the vectors of the noise and the one-hot labels for the generator\n",
        "            noise_and_labels = combine_vectors(fake_noise, one_hot_labels)\n",
        "            fake = gen(noise_and_labels)\n",
        "            # Combine the vectors of the images and the one-hot labels for the discriminator\n",
        "            fake_image_and_labels = combine_vectors(fake.detach(), image_one_hot_labels)\n",
        "            real_image_and_labels = combine_vectors(real, image_one_hot_labels)\n",
        "            disc_fake_pred = disc(fake_image_and_labels)\n",
        "            disc_real_pred = disc(real_image_and_labels)\n",
        "\n",
        "            disc_fake_loss = criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred))\n",
        "            disc_real_loss = criterion(disc_real_pred, torch.ones_like(disc_real_pred))\n",
        "            disc_loss = (disc_fake_loss + disc_real_loss) / 2\n",
        "            disc_loss.backward(retain_graph=True)\n",
        "            disc_opt.step()\n",
        "\n",
        "            # Keep track of the average discriminator loss\n",
        "            mean_discriminator_loss += disc_loss.item() / display_step\n",
        "\n",
        "            ### Update generator ###\n",
        "            # Zero out the generator gradients\n",
        "            gen_opt.zero_grad()\n",
        "\n",
        "            # Pass the discriminator the combination of the fake images and the one-hot labels\n",
        "            fake_image_and_labels = combine_vectors(fake, image_one_hot_labels)\n",
        "\n",
        "            disc_fake_pred = disc(fake_image_and_labels)\n",
        "            gen_loss = criterion(disc_fake_pred, torch.ones_like(disc_fake_pred))\n",
        "            gen_loss.backward()\n",
        "            gen_opt.step()\n",
        "\n",
        "            # Keep track of the average generator loss\n",
        "            mean_generator_loss += gen_loss.item() / display_step\n",
        "\n",
        "            if cur_step % display_step == 0 and cur_step > 0:\n",
        "                print(f\"Step {cur_step}: Generator loss: {mean_generator_loss}, discriminator loss: {mean_discriminator_loss}\")\n",
        "                show_tensor_images(fake)\n",
        "                show_tensor_images(real)\n",
        "                mean_generator_loss = 0\n",
        "                mean_discriminator_loss = 0\n",
        "            cur_step += 1\n",
        "\n",
        "def train_classifier():\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    n_epochs = 10\n",
        "\n",
        "    validation_dataloader = DataLoader(\n",
        "        CIFAR100(\".\", train=False, download=True, transform=transform),\n",
        "        batch_size=batch_size)\n",
        "\n",
        "    display_step = 10\n",
        "    batch_size = 512\n",
        "    lr = 0.0002\n",
        "    device = 'cuda'\n",
        "    classifier = Classifier(cifar100_shape[0], n_classes).to(device)\n",
        "    classifier_opt = torch.optim.Adam(classifier.parameters(), lr=lr)\n",
        "    cur_step = 0\n",
        "    for epoch in range(n_epochs):\n",
        "        for real, labels in tqdm(dataloader):\n",
        "            cur_batch_size = len(real)\n",
        "            real = real.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            ### Update classifier ###\n",
        "            # Get noise corresponding to the current batch_size\n",
        "            classifier_opt.zero_grad()\n",
        "            labels_hat = classifier(real.detach())\n",
        "            classifier_loss = criterion(labels_hat, labels)\n",
        "            classifier_loss.backward()\n",
        "            classifier_opt.step()\n",
        "\n",
        "            if cur_step % display_step == 0:\n",
        "                classifier_val_loss = 0\n",
        "                classifier_correct = 0\n",
        "                num_validation = 0\n",
        "                for val_example, val_label in validation_dataloader:\n",
        "                    cur_batch_size = len(val_example)\n",
        "                    num_validation += cur_batch_size\n",
        "                    val_example = val_example.to(device)\n",
        "                    val_label = val_label.to(device)\n",
        "                    labels_hat = classifier(val_example)\n",
        "                    classifier_val_loss += criterion(labels_hat, val_label) * cur_batch_size\n",
        "                    classifier_correct += (labels_hat.argmax(1) == val_label).float().sum()\n",
        "\n",
        "                print(f\"Step {cur_step}: \"\n",
        "                        f\"Classifier loss: {classifier_val_loss.item() / num_validation}, \"\n",
        "                        f\"classifier accuracy: {classifier_correct.item() / num_validation}\")\n",
        "            cur_step += 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYGOiy-xWHOH"
      },
      "source": [
        "## Tuning the Classifier\n",
        "After two courses, you've probably had some fun debugging your GANs and have started to consider yourself a bug master. For this assignment, your mastery will be put to the test on some interesting bugs... well, bugs as in insects.\n",
        "\n",
        "As a bug master, you want a classifier capable of classifying different species of bugs: bees, beetles, butterflies, caterpillar, and more. Luckily, you found a great dataset with a lot of animal species and objects, and you trained your classifier on that.\n",
        "\n",
        "But the bug classes don't do as well as you would like. Now your plan is to train a GAN on the same data so it can generate new bugs to make your classifier better at distinguishing between all of your favorite bugs!\n",
        "\n",
        "You will fine-tune your model by augmenting the original real data with fake data and during that process, observe how to increase the accuracy of your classifier with these fake, GAN-generated bugs. After this, you will prove your worth as a bug master."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSuAJTuYYr2o"
      },
      "source": [
        "#### Sampling Ratio\n",
        "\n",
        "Suppose that you've decided that although you have this pre-trained general generator and this general classifier, capable of identifying 100 classes with some accuracy (~17%), what you'd really like is a model that can classify the five different kinds of bugs in the dataset. You'll fine-tune your model by augmenting your data with the generated images. Keep in mind that both the generator and the classifier were trained on the same images: the 40 images per class you painstakingly found so your generator may not be great. This is the caveat with data augmentation, ultimately you are still bound by the real data that you have but you want to try and create more. To make your models even better, you would need to take some more bug photos, label them, and add them to your training set and/or use higher quality photos.\n",
        "\n",
        "To start, you'll first need to write some code to sample a combination of real and generated images. Given a probability, `p_real`, you'll need to generate a combined tensor where roughly `p_real` of the returned images are sampled from the real images. Note that you should not interpolate the images here: you should choose each image from the real or fake set with a given probability. For example, if your real images are a tensor of `[[1, 2, 3, 4, 5]]` and your fake images are a tensor of `[[-1, -2, -3, -4, -5]]`, and `p_real = 0.2`, two potential return values are `[[1, -2, 3, -4, -5]]` or `[[-1, 2, -3, -4, -5]]`\n",
        "\n",
        "In addition, we will expect the images to remain in the same order to maintain their alignment with their labels (this applies to the fake images too!).\n",
        "\n",
        "<details>\n",
        "<summary>\n",
        "<font size=\"3\" color=\"green\">\n",
        "<b>Optional hints for <code><font size=\"4\">combine_sample</font></code></b>\n",
        "</font>\n",
        "</summary>\n",
        "\n",
        "1.   This code probably shouldn't be much longer than 3 lines\n",
        "2.   You can index using a set of booleans which have the same length as your tensor\n",
        "3.   You want to generate an unbiased sample, which you can do (for example) with `torch.rand(length_reals) > p`.\n",
        "4.   There are many approaches here that will give a correct answer here. You may find [`torch.rand`](https://pytorch.org/docs/stable/generated/torch.rand.html) or [`torch.bernoulli`](https://pytorch.org/docs/master/generated/torch.bernoulli.html) useful.\n",
        "5.   You don't want to edit an argument in place, so you may find [`cur_tensor.clone()`](https://pytorch.org/docs/stable/tensors.html) useful too, which makes a copy of `cur_tensor`.\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16JJ7RlKxrsY"
      },
      "outputs": [],
      "source": [
        "def combine_sample(real, fake, p_real):\n",
        "    mask = torch.bernoulli(torch.ones((real.shape[0]))*p_real).int()\n",
        "    while mask.dim() < real.dim(): mask.unsqueeze_(1)  # alterantive to mask[:,None] or mask[:,None,None]\n",
        "    target_images = (real * mask + fake * (1-mask)).reshape(real.shape)\n",
        "    return target_images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpMGXMYU1a4O"
      },
      "source": [
        "Now you have a challenge: find a `p_real` and a generator image such that your classifier gets an average of a 51% accuracy or higher on the insects, when evaluated with the `eval_augmentation` function. **You'll need to fill in `find_optimal` to find these parameters to solve this part!** Note that if your answer takes a very long time to run, you may need to hard-code the solution it finds.\n",
        "\n",
        "When you're training a generator, you will often have to look at different checkpoints and choose one that does the best (either empirically or using some evaluation method). Here, you are given four generator checkpoints: `gen_1.pt`, `gen_2.pt`, `gen_3.pt`, `gen_4.pt`. You'll also have some scratch area to write whatever code you'd like to solve this problem, but you must return a `p_real` and an image name of your selected generator checkpoint. You can hard-code/brute-force these numbers if you would like, but you are encouraged to try to solve this problem in a more general way. In practice, you would also want a test set (since it is possible to overfit on a validation set), but for simplicity you can just focus on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fc7mFIVRVT_2"
      },
      "outputs": [],
      "source": [
        "def find_optimal():\n",
        "    gen_names = [\n",
        "        \"gen_1.pt\",\n",
        "        \"gen_2.pt\",\n",
        "        \"gen_3.pt\",\n",
        "        \"gen_4.pt\"\n",
        "    ]\n",
        "\n",
        "    best_score = 0\n",
        "    best_p_real, best_gen_name = 0.0, gen_names[0]\n",
        "    for p_real in [ 0.15, 0.3, 0.5, 0.65, 0.85 ]:\n",
        "        for gen_name in gen_names:\n",
        "            score = eval_augmentation(p_real, gen_name, n_test=1)\n",
        "            if score > best_score:\n",
        "                best_p_real, best_gen_name = p_real, gen_name\n",
        "                print( p_real, gen_name, score )\n",
        "            if score > 0.75:\n",
        "                break\n",
        "    return best_p_real, best_gen_name\n",
        "\n",
        "def augmented_train(p_real, gen_name):\n",
        "    gen = Generator(generator_input_dim).to(device)\n",
        "    gen.load_state_dict(torch.load(gen_name))\n",
        "\n",
        "    classifier = Classifier(cifar100_shape[0], n_classes).to(device)\n",
        "    classifier.load_state_dict(torch.load(\"class.pt\"))\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    batch_size = 256\n",
        "\n",
        "    train_set = torch.load(\"insect_train.pt\")\n",
        "    val_set = torch.load(\"insect_val.pt\")\n",
        "    dataloader = DataLoader(\n",
        "        torch.utils.data.TensorDataset(train_set[\"images\"], train_set[\"labels\"]),\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True\n",
        "    )\n",
        "    validation_dataloader = DataLoader(\n",
        "        torch.utils.data.TensorDataset(val_set[\"images\"], val_set[\"labels\"]),\n",
        "        batch_size=batch_size\n",
        "    )\n",
        "\n",
        "    display_step = 1\n",
        "    lr = 0.0002\n",
        "    n_epochs = 20\n",
        "    classifier_opt = torch.optim.Adam(classifier.parameters(), lr=lr)\n",
        "    cur_step = 0\n",
        "    best_score = 0\n",
        "    for epoch in range(n_epochs):\n",
        "        for real, labels in dataloader:\n",
        "            real = real.to(device)\n",
        "            # Flatten the image\n",
        "            labels = labels.to(device)\n",
        "            one_hot_labels = get_one_hot_labels(labels.to(device), n_classes).float()\n",
        "\n",
        "            ### Update classifier ###\n",
        "            # Get noise corresponding to the current batch_size\n",
        "            classifier_opt.zero_grad()\n",
        "            cur_batch_size = len(labels)\n",
        "            fake_noise = get_noise(cur_batch_size, z_dim, device=device)\n",
        "            noise_and_labels = combine_vectors(fake_noise, one_hot_labels)\n",
        "            fake = gen(noise_and_labels)\n",
        "\n",
        "            target_images = combine_sample(real.clone(), fake.clone(), p_real)\n",
        "            labels_hat = classifier(target_images.detach())\n",
        "            classifier_loss = criterion(labels_hat, labels)\n",
        "            classifier_loss.backward()\n",
        "            classifier_opt.step()\n",
        "\n",
        "            # Calculate the accuracy on the validation set\n",
        "            if cur_step % display_step == 0 and cur_step > 0:\n",
        "                classifier_val_loss = 0\n",
        "                classifier_correct = 0\n",
        "                num_validation = 0\n",
        "                with torch.no_grad():\n",
        "                    for val_example, val_label in validation_dataloader:\n",
        "                        cur_batch_size = len(val_example)\n",
        "                        num_validation += cur_batch_size\n",
        "                        val_example = val_example.to(device)\n",
        "                        val_label = val_label.to(device)\n",
        "                        labels_hat = classifier(val_example)\n",
        "                        classifier_val_loss += criterion(labels_hat, val_label) * cur_batch_size\n",
        "                        classifier_correct += (labels_hat.argmax(1) == val_label).float().sum()\n",
        "                    accuracy = classifier_correct.item() / num_validation\n",
        "                    if accuracy > best_score:\n",
        "                        best_score = accuracy\n",
        "            cur_step += 1\n",
        "    return best_score\n",
        "\n",
        "def eval_augmentation(p_real, gen_name, n_test=20):\n",
        "    total = 0\n",
        "    for i in range(n_test):\n",
        "        total += augmented_train(p_real, gen_name)\n",
        "    return total / n_test\n",
        "\n",
        "best_p_real, best_gen_name = find_optimal()\n",
        "performance = eval_augmentation(best_p_real, best_gen_name)\n",
        "print(f\"Your model had an accuracy of {performance:0.1%}\")\n",
        "assert performance > 0.51\n",
        "print(\"Success!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmqeeBjE32ls"
      },
      "source": [
        "You'll likely find that the worst performance is when the generator is performing alone: this corresponds to the case where you might be trying to hide the underlying examples from the classifier. Perhaps you don't want other people to know about your specific bugs!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLRFjtb_HEuP"
      },
      "outputs": [],
      "source": [
        "accuracies = []\n",
        "p_real_all = torch.linspace(0, 1, 21)\n",
        "for p_real_vis in tqdm(p_real_all):\n",
        "    accuracies += [eval_augmentation(p_real_vis, best_gen_name, n_test=4)]\n",
        "plt.plot(p_real_all.tolist(), accuracies)\n",
        "plt.ylabel(\"Accuracy\")\n",
        "_ = plt.xlabel(\"Percent Real Images\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2j-xodd1ykT"
      },
      "source": [
        "Here's a visualization of what the generator is actually generating, with real examples of each class above the corresponding generated image.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HpcnjIK_0WdF"
      },
      "outputs": [],
      "source": [
        "examples = [4, 41, 80, 122, 160]\n",
        "train_images = torch.load(\"insect_train.pt\")[\"images\"][examples]\n",
        "train_labels = torch.load(\"insect_train.pt\")[\"labels\"][examples]\n",
        "\n",
        "one_hot_labels = get_one_hot_labels(train_labels.to(device), n_classes).float()\n",
        "fake_noise = get_noise(len(train_images), z_dim, device=device)\n",
        "noise_and_labels = combine_vectors(fake_noise, one_hot_labels)\n",
        "gen = Generator(generator_input_dim).to(device)\n",
        "gen.load_state_dict(torch.load(best_gen_name))\n",
        "\n",
        "fake = gen(noise_and_labels)\n",
        "show_tensor_images(torch.cat([train_images.cpu(), fake.cpu()]))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "P1A1M6kpnfxw",
        "ccQZRSYFXsHh",
        "tYXJTxM9pzZK"
      ],
      "provenance": []
    },
    "coursera": {
      "schema_names": [
        "GANSC3-1A"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}