{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1k6Y9afpxL6"
      },
      "source": [
        "# Intro\n",
        "\n",
        "[PyTorch](https://pytorch.org/) is a very powerful, popular and well-maintained DL frameworks. In addition to being used by DL engineers in industry, PyTorch is a favored tool amongst researchers. Many deep learning papers are published using PyTorch. It is designed to be intuitive and user-friendly, sharing a lot of common ground with the Python library NumPy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwp6T5ZMteDC"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvXp0rlPBqdQ"
      },
      "source": [
        "# Why PyTorch?\n",
        "\n",
        "One important question worth asking is, why is PyTorch being used for this course? There is a great breakdown by [the Gradient](https://thegradient.pub/state-of-ml-frameworks-2019-pytorch-dominates-research-tensorflow-dominates-industry/) looking at the state of machine learning frameworks today. In part, as highlighted by the article, PyTorch is generally more pythonic than alternative frameworks, easier to debug, and is the most-used language in machine learning research by a large and growing margin. While PyTorch's primary alternative, Tensorflow, has attempted to integrate many of PyTorch's features, Tensorflow's implementations come with some inherent limitations highlighted in the article.\n",
        "\n",
        "Notably, while PyTorch's industry usage has grown, Tensorflow is still (for now) a slight favorite in industry. In practice, the features that make PyTorch attractive for research also make it attractive for education, and the general trend of machine learning research and practice to PyTorch makes it the more proactive choice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCgwdP20r1yX"
      },
      "source": [
        "# Tensor Properties\n",
        "The fundamental data structure in PyTorch is called a [tensors](https://pytorch.org/docs/stable/tensors.html). A tensor is essentially an array, which can support many mathematical operations, and will form a building block for our NNs.\n",
        "\n",
        "\n",
        "One way to create tensors from a list or an array is to use `torch.Tensor`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0hgYekGsxlB"
      },
      "outputs": [],
      "source": [
        "example_tensor = torch.Tensor(\n",
        "    [\n",
        "     [[1, 2], [3, 4]],\n",
        "     [[5, 6], [7, 8]],\n",
        "     [[9, 0], [1, 2]]\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "np_array = np.array(    [\n",
        "     [[1, 2], [3, 4]],\n",
        "     [[5, 6], [7, 8]],\n",
        "     [[9, 0], [1, 2]]\n",
        "    ]\n",
        ")\n",
        "\n",
        "np_tensor = torch.from_numpy(np_array)"
      ],
      "metadata": {
        "id": "6azCHFI7NKxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch also supports tensor creation directly from NumPy arrays, using torch.from_numpy(). torch.tensor() will work directly on NumPy arrays too. Like NumPy arrays, tensors are multidimensional, representing a collection of elements arranged in a grid with multiple dimensions."
      ],
      "metadata": {
        "id": "HSlZWMIVMtMc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dO4C2oft7zq"
      },
      "source": [
        "You can view the tensor in the notebook by simple printing it out (though some larger tensors will be cut off)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "id": "U2FKEzeYuEOX",
        "outputId": "dfa12ff7-afd1-4737-a669-54f36b4209dd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[1., 2.],\n",
              "         [3., 4.]],\n",
              "\n",
              "        [[5., 6.],\n",
              "         [7., 8.]],\n",
              "\n",
              "        [[9., 0.],\n",
              "         [1., 2.]]])"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example_tensor"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np_tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9cbQ1QANuVP",
        "outputId": "0285f23a-eb82-44a2-d9b3-22e6bdff11a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[1, 2],\n",
              "         [3, 4]],\n",
              "\n",
              "        [[5, 6],\n",
              "         [7, 8]],\n",
              "\n",
              "        [[9, 0],\n",
              "         [1, 2]]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUwlmUngw-VR"
      },
      "source": [
        "## Tensor Properties: Device\n",
        "\n",
        "One important property is the device of the tensor - throughout this notebook you'll be sticking to tensors which are on the CPU. However, throughout the course you'll also be using tensors on GPU (that is, a graphics card which will be provided for you to use for the course). To view the device of the tensor, all you need to write is `example_tensor.device`. To move a tensor to a new device, you can write `new_tensor = example_tensor.to(device)` where device will be either `cpu` or `cuda`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "R7SF44_Vw9h0",
        "outputId": "57f90e38-f9e1-4115-8f27-ebe651d5b2fa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example_tensor.device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkfySyFduHQi"
      },
      "source": [
        "## Tensor Properties: Shape\n",
        "\n",
        "And you can get the number of elements in each dimension by printing out the tensor's shape, using `example_tensor.shape`, something you're likely familiar with if you've used numpy. For example, this tensor is a $3\\times2\\times2$ tensor, since it has 3 elements, each of which are $2\\times2$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "DKmfzpOBun0t",
        "outputId": "883009b6-7300-4329-f9ec-df99cc36d846"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([3, 2, 2])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example_tensor.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensor Properties: Types\n"
      ],
      "metadata": {
        "id": "IqaiDdftOAV9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_tensor.dtype"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peCiC-RZOAh0",
        "outputId": "2bb3c271-f580-45a1-8427-d193381b5f2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.float32"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aL954xmAuq4b"
      },
      "source": [
        "You can also get the size of a particular dimension $n$ using `example_tensor.shape[n]` or equivalently `example_tensor.size(n)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "7IKy3BB8uqBo",
        "outputId": "7fac1275-132f-4d2b-bf63-73065a2aea6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shape[0] = 3\n",
            "size(1) = 2\n"
          ]
        }
      ],
      "source": [
        "print(\"shape[0] =\", example_tensor.shape[0])\n",
        "print(\"size(1) =\", example_tensor.size(1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pzzG8bav5rl"
      },
      "source": [
        "Finally, it is sometimes useful to get the number of dimensions (rank) or the number of elements, which you can do as follows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "l_j9qTwyv41-",
        "outputId": "5921cbd1-19a2-4543-9488-3f72c0cb4970"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rank = 3\n",
            "Number of elements = 12\n"
          ]
        }
      ],
      "source": [
        "print(\"Rank =\", len(example_tensor.shape))\n",
        "print(\"Number of elements =\", example_tensor.numel())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensor Operations: Add and subtract.\n",
        "PyTorch tensors support several operations similar to NumPy arrays. We can add or subtract tensors, provided that their shapes are compatible. When shapes are incompatible, we get an error."
      ],
      "metadata": {
        "id": "0gxWYBsgORRk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a= torch.tensor([[1, 2], [3, 4]])\n",
        "b= torch.tensor([[5, 6], [7, 8]])\n",
        "a + b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJCQ2i9uOb-P",
        "outputId": "a8580654-872b-4c43-c556-19a54a662129"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 6,  8],\n",
              "        [10, 12]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a= torch.tensor([[1, 2], [3, 4], [5, 6]])\n",
        "b= torch.tensor([[5, 6], [7, 8]])\n",
        "\n",
        "#a + b -----> an error"
      ],
      "metadata": {
        "id": "6UMFpGa5O360"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensor Operation: Element-Wise multiplication\n",
        "\n",
        "We can also perform element-wise multiplication, which involves multiplying each corresponding element from 2 arrays of the same shape, and many other operations, such as tensor transposition, matrix multiplication, and tensor concatenation. We'll get into these later. Most operations available for NumPy arrays can be performed on PyTorch tensors."
      ],
      "metadata": {
        "id": "t8wBLOWfPE08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a * b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNCXIVc-PFRJ",
        "outputId": "27116762-30fd-457c-c06f-e4da49d5bcff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 5, 12],\n",
              "        [21, 32]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gibyKQJQzLkm"
      },
      "source": [
        "# Indexing Tensors\n",
        "\n",
        "As with numpy, you can access specific elements or subsets of elements of a tensor. To access the $n$-th element, you can simply write `example_tensor[n]` - as with Python in general, these dimensions are 0-indexed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "F87bFA5SzNz7",
        "outputId": "1b0a8381-6fd8-40b4-a5c8-88cc80029f8e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[5., 6.],\n",
              "        [7., 8.]])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example_tensor[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CegGw5wzpGa"
      },
      "source": [
        "In addition, if you want to access the $j$-th dimension of the $i$-th example, you can write `example_tensor[i, j]`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "bl1JSZcRz0xn",
        "outputId": "7f98e47b-66cb-4927-b784-7e4bcb9eb687"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(7.)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example_tensor[1, 1, 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyQRCRIa4NaY"
      },
      "source": [
        "Note that if you'd like to get a Python scalar value from a tensor, you can use `example_scalar.item()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "e56KSJOq4YOE",
        "outputId": "29e1fd13-32df-40c5-e558-3193fa5da629"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "7.0"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example_scalar = example_tensor[1, 1, 0]\n",
        "example_scalar.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZdMEQfu0A7h"
      },
      "source": [
        "In addition, you can index into the ith element of a column by using `x[:, i]`. For example, if you want the top-left element of each element in `example_tensor`, which is the `0, 0` element of each matrix, you can write:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "x2cFxJx50eGH",
        "outputId": "e66eade9-4b4b-4c7a-ea99-a83195d10541"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([1., 5., 9.])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example_tensor[:, 0, 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-rTBP-1whd2"
      },
      "source": [
        "# Initializing Tensors\n",
        "\n",
        "There are many ways to create new tensors in PyTorch, but in this course, the most important ones are:\n",
        "\n",
        "[`torch.ones_like`](https://pytorch.org/docs/master/generated/torch.ones_like.html): creates a tensor of all ones with the same shape and device as `example_tensor`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "id": "g7gbs4AnwlIo",
        "outputId": "b0c67ed9-e33f-47d6-d95c-e53bc4f90dec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[1., 1.],\n",
              "         [1., 1.]],\n",
              "\n",
              "        [[1., 1.],\n",
              "         [1., 1.]],\n",
              "\n",
              "        [[1., 1.],\n",
              "         [1., 1.]]])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.ones_like(example_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aIbSlaJy9Z0"
      },
      "source": [
        "[`torch.zeros_like`](https://pytorch.org/docs/master/generated/torch.zeros_like.html): creates a tensor of all zeros with the same shape and device as `example_tensor`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "id": "X4cWQduzzCd8",
        "outputId": "dbc8a5fa-8db1-4f6d-e38e-d1deb982ff36"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[0., 0.],\n",
              "         [0., 0.]],\n",
              "\n",
              "        [[0., 0.],\n",
              "         [0., 0.]],\n",
              "\n",
              "        [[0., 0.],\n",
              "         [0., 0.]]])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.zeros_like(example_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsOmgS1izDS_"
      },
      "source": [
        "[`torch.randn_like`](https://pytorch.org/docs/stable/generated/torch.randn_like.html): creates a tensor with every element sampled from a [Normal (or Gaussian) distribution](https://en.wikipedia.org/wiki/Normal_distribution) with the same shape and device as `example_tensor`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "id": "2hto51IazDow",
        "outputId": "cb62a68a-6171-4d1e-eb9b-f31784464aac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[-0.3675,  0.2242],\n",
              "         [-0.3378, -1.0944]],\n",
              "\n",
              "        [[ 1.5371,  0.7701],\n",
              "         [-0.1490, -0.0928]],\n",
              "\n",
              "        [[ 0.3270,  0.4642],\n",
              "         [ 0.1494,  0.1283]]])"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.randn_like(example_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXp0i5Cf6AGj"
      },
      "source": [
        "Sometimes (though less often than you'd expect), you might need to initialize a tensor knowing only the shape and device, without a tensor for reference for `ones_like` or `randn_like`. In this case, you can create a $2x2$ tensor as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "RZRqt3-S6cUZ",
        "outputId": "7bef97cc-a303-4200-c0f8-ef9bf3cb4996"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 0.2235, -1.8912],\n",
              "        [-1.2873,  0.7405]])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.randn(2, 2, device='cpu') # Alternatively, for a GPU tensor, you'd use device='cuda'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTkmDwVsrM6R"
      },
      "source": [
        "# Basic Functions\n",
        "\n",
        "There are a number of basic functions that you should know to use PyTorch - if you're familiar with numpy, all commonly-used functions exist in PyTorch, usually with the same name. You can perform element-wise multiplication / division by a scalar $c$ by simply writing `c * example_tensor`, and element-wise addition / subtraction by a scalar by writing `example_tensor + c`\n",
        "\n",
        "Note that most operations are not in-place in PyTorch, which means that they don't change the original variable's data (However, you can reassign the same variable name to the changed data if you'd like, such as `example_tensor = example_tensor + 1`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "id": "FpfwOUdopsF_",
        "outputId": "32347400-2e6a-40c6-e6f1-21e6aacde795"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[ -8.,  -6.],\n",
              "         [ -4.,  -2.]],\n",
              "\n",
              "        [[  0.,   2.],\n",
              "         [  4.,   6.]],\n",
              "\n",
              "        [[  8., -10.],\n",
              "         [ -8.,  -6.]]])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(example_tensor - 5) * 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uciZnx4b3UjX"
      },
      "source": [
        "You can calculate the mean or standard deviation of a tensor using [`example_tensor.mean()`](https://pytorch.org/docs/stable/generated/torch.mean.html) or [`example_tensor.std()`](https://pytorch.org/docs/stable/generated/torch.std.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "0ELXUKG7329z",
        "outputId": "720dd190-7dd4-43f1-e53c-cba4263eb2be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean: tensor(4.)\n",
            "Stdev: tensor(2.9848)\n"
          ]
        }
      ],
      "source": [
        "print(\"Mean:\", example_tensor.mean())\n",
        "print(\"Stdev:\", example_tensor.std())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QsyTRym32SX"
      },
      "source": [
        "You might also want to find the mean or standard deviation along a particular dimension. To do this you can simple pass the number corresponding to that dimension to the function. For example, if you want to get the average $2\\times2$ matrix of the $3\\times2\\times2$ `example_tensor` you can write:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "eCJl3Im25B9k",
        "outputId": "4bd9decd-579e-462c-bde1-ee8d9d1b2061"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[5.0000, 2.6667],\n",
              "        [3.6667, 4.6667]])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example_tensor.mean(0)\n",
        "\n",
        "# Equivalently, you could also write:\n",
        "# example_tensor.mean(dim=0)\n",
        "# example_tensor.mean(axis=0)\n",
        "# torch.mean(example_tensor, 0)\n",
        "# torch.mean(example_tensor, dim=0)\n",
        "# torch.mean(example_tensor, axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vb-_5ubc8t97"
      },
      "source": [
        "PyTorch has many other powerful functions but these should be all of PyTorch functions you need for this course outside of its neural network module (`torch.nn`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtWjExD69JEs"
      },
      "source": [
        "# PyTorch Neural Network Module (`torch.nn`)\n",
        "\n",
        "PyTorch has a lot of powerful classes in its `torch.nn` module (Usually, imported as simply `nn`). These classes allow you to create a new function which transforms a tensor in specific way, often retaining information when called multiple times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYrgloYo_slC"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyCPVmTD_kkl"
      },
      "source": [
        "## `nn.Linear`\n",
        "\n",
        "To create a linear layer, you need to pass it the number of input dimensions and the number of output dimensions. The linear object initialized as `nn.Linear(10, 2)` will take in a $n\\times10$ matrix and return an $n\\times2$ matrix, where all $n$ elements have had the same linear transformation performed. For example, you can initialize a linear layer which performs the operation $Ax + b$, where $A$ and $b$ are initialized randomly when you generate the [`nn.Linear()`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) object.\n",
        "\n",
        "A linear layer takes an input tensor, applies a linear function to it, and returns an output. nn.Linear() takes two arguments: in_features, which is the number of features in our input (three), and out_features, specifying the desired size of the output tensor (in this case two). Correctly specifying in_features ensures our linear layer can receive the input_tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNPaHPo89VrN",
        "outputId": "53468a68-9d4f-45c9-e9b3-8fa276276486"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.5202,  1.0930],\n",
              "        [-0.3421, -1.4586],\n",
              "        [-0.2268,  0.4987]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "linear = nn.Linear(10, 2)\n",
        "example_input = torch.randn(3, 10)\n",
        "example_output = linear(example_input)\n",
        "example_output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Each linear layer has a set of weights and biases associated with it."
      ],
      "metadata": {
        "id": "C52sKM6tQuR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "linear.weight"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRpNKts3Q0EO",
        "outputId": "c00e0d68-8b94-4f4d-af1f-5be2cdc12e9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[ 0.0230,  0.2199, -0.1194, -0.1274,  0.3104, -0.2273, -0.1317, -0.2265,\n",
              "         -0.1590,  0.2325],\n",
              "        [-0.3062,  0.0826, -0.2638,  0.1689,  0.2513, -0.0459, -0.1621, -0.0443,\n",
              "         -0.2538, -0.1470]], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "linear.bias"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWU32vthQ7zL",
        "outputId": "c911790c-cdea-4a46-be9d-a7469a71f2e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([ 0.1897, -0.1656], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGNULkJR_mzn"
      },
      "source": [
        "## `nn.ReLU`\n",
        "\n",
        "[`nn.ReLU()`](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html) will create an object that, when receiving a tensor, will perform a ReLU activation function. This will be reviewed further in lecture, but in essence, a ReLU non-linearity sets all negative numbers in a tensor to zero. In general, the simplest neural networks are composed of series of linear transformations, each followed by activation functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "nGxVFS3nBASc",
        "outputId": "d5f57584-1bad-4803-ba8c-b69881db4a1f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.2900, 0.0000],\n",
              "        [0.4298, 0.4173],\n",
              "        [0.4861, 0.0000]], grad_fn=<ReluBackward0>)"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "relu = nn.ReLU()\n",
        "relu_output = relu(example_output)\n",
        "relu_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzfOEZ03AJzA"
      },
      "source": [
        "## `nn.BatchNorm1d`\n",
        "\n",
        "[`nn.BatchNorm1d`](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html) is a normalization technique that will rescale a batch of $n$ inputs to have a consistent mean and standard deviation between batches.  \n",
        "\n",
        "As indicated by the `1d` in its name, this is for situations where you expects a set of inputs, where each of them is a flat list of numbers. In other words, each input is a vector, not a matrix or higher-dimensional tensor. For a set of images, each of which is a higher-dimensional tensor, you'd use [`nn.BatchNorm2d`](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html), discussed later on this page.\n",
        "\n",
        "`nn.BatchNorm1d` takes an argument of the number of input dimensions of each object in the batch (the size of each example vector)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "O4tYsi9-G9vM",
        "outputId": "ba61d37c-a8af-4663-fcc2-1691c6d241de"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-1.3570, -0.7070],\n",
              "        [ 0.3368,  1.4140],\n",
              "        [ 1.0202, -0.7070]], grad_fn=<NativeBatchNormBackward>)"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batchnorm = nn.BatchNorm1d(2)\n",
        "batchnorm_output = batchnorm(relu_output)\n",
        "batchnorm_output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What if we wanted to stack multiple layers? Meet nn.Sequential(), a PyTorch container that allows us to stack multiple neural network modules in sequence. The code provided shows three linear layers stacked within nn.Sequential(). This model takes input, passes it to each linear layer in sequence, and returns output."
      ],
      "metadata": {
        "id": "_FooJhSWRMvi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMZewDz9Idr1"
      },
      "source": [
        "## `nn.Sequential`\n",
        "\n",
        "[`nn.Sequential`](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) creates a single operation that performs a sequence of operations. For example, you can write a neural network layer with a batch normalization as"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "R3GhASjyJt3N",
        "outputId": "3ef779ca-a17b-42fd-f2e5-fbb5fdc60b13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input: \n",
            "tensor([[ 1.7690,  0.2864,  0.7925,  2.2849,  1.5226],\n",
            "        [ 0.1877,  0.1367, -0.2833,  2.0905,  0.0454],\n",
            "        [ 0.7825,  2.2969,  1.2144,  0.2526,  2.5709],\n",
            "        [-0.4878,  1.9587,  1.6849,  0.5284,  1.9027],\n",
            "        [ 0.5384,  1.1787,  0.4961, -1.6326,  1.4192]])\n",
            "output: \n",
            "tensor([[0.0000, 1.1865],\n",
            "        [1.5208, 0.0000],\n",
            "        [0.0000, 1.1601],\n",
            "        [0.0000, 0.0000],\n",
            "        [0.7246, 0.0000]], grad_fn=<ReluBackward0>)\n"
          ]
        }
      ],
      "source": [
        "mlp_layer = nn.Sequential(\n",
        "    nn.Linear(5, 2),\n",
        "    nn.BatchNorm1d(2),\n",
        "    nn.ReLU()\n",
        ")\n",
        "\n",
        "test_example = torch.randn(5,5) + 1\n",
        "print(\"input: \")\n",
        "print(test_example)\n",
        "print(\"output: \")\n",
        "print(mlp_layer(test_example))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SToQiSv5K5Yb"
      },
      "source": [
        "# Optimization\n",
        "\n",
        "One of the most important aspects of essentially any machine learning framework is its automatic differentiation library."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4GZFCZ0QqI1"
      },
      "source": [
        "## Optimizers\n",
        "\n",
        "To create an optimizer in PyTorch, you'll need to use the `torch.optim` module, often imported as `optim`. [`optim.Adam`](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam) corresponds to the Adam optimizer. To create an optimizer object, you'll need to pass it the parameters to be optimized and the learning rate, `lr`, as well as any other parameters specific to the optimizer.\n",
        "\n",
        "For all `nn` objects, you can access their parameters as a list using their `parameters()` method, as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIcCbs35K4wY"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "adam_opt = optim.Adam(mlp_layer.parameters(), lr=1e-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BsPFZu2M0Xx"
      },
      "source": [
        "## Training Loop\n",
        "\n",
        "To train a neural network in PyTorch, you will first need to understand the job of a loss function. You will then realize that training a network requires minimizing that loss function, which is done by calculating gradients. You will learn how to use these gradients to update your model's parameters, and finally, you will write your first training loop.\n",
        "\n",
        "A (basic) training step in PyTorch consists of four basic parts:\n",
        "\n",
        "\n",
        "1.   Set all of the gradients to zero using `opt.zero_grad()`\n",
        "2.   Calculate the loss, `loss`\n",
        "3.   Calculate the gradients with respect to the loss using `loss.backward()`\n",
        "4.   Update the parameters being optimized using `opt.step()`\n",
        "\n",
        "That might look like the following code (and you'll notice that if you run it several times, the loss goes down):\n",
        "\n",
        "If we create a model, choose a loss function, create a dataset and define an optimizer, we are finally in a position to train a model. We can then loop over each element of the dataset, calculate the loss, compute local gradients and use them to update model parameters. This final step is repeated a certain number of times, and is called the training loop. In scikit-learn, the training loop is located in the .fit() method of the model. ***We'll have to implement our own training loop in PyTorch.***\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "zm6lPx4sOJht",
        "outputId": "c21672bd-a306-42ab-face-9a299511a059"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0.7719, grad_fn=<MeanBackward0>)\n"
          ]
        }
      ],
      "source": [
        "train_example = torch.randn(100,5) + 1\n",
        "adam_opt.zero_grad()\n",
        "\n",
        "# We'll use a simple loss function of mean distance from 1\n",
        "# torch.abs takes the absolute value of a tensor\n",
        "cur_loss = torch.abs(1 - mlp_layer(train_example)).mean()\n",
        "\n",
        "cur_loss.backward()\n",
        "adam_opt.step()\n",
        "print(cur_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDjhZBCeTc6o"
      },
      "source": [
        "## `requires_grad_()`\n",
        "\n",
        "You can also tell PyTorch that it needs to calculate the gradient with respect to a tensor that you created by saying `example_tensor.requires_grad_()`, which will change it in-place. This means that even if PyTorch wouldn't normally store a grad for that particular tensor, it will for that specified tensor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mB22ovHyUEvH"
      },
      "source": [
        "## `with torch.no_grad():`\n",
        "\n",
        "PyTorch will usually calculate the gradients as it proceeds through a set of operations on tensors. This can often take up unnecessary computations and memory, especially if you're performing an evaluation. However, you can wrap a piece of code with `with torch.no_grad()` to prevent the gradients from being calculated in a piece of code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kowb1M425CE_"
      },
      "source": [
        "\n",
        "## `detach():`\n",
        "\n",
        "Sometimes, you want to calculate and use a tensor's value without calculating its gradients. For example, if you have two models, A and B, and you want to directly optimize the parameters of A with respect to the output of B, without calculating the gradients through B, then you could feed the detached output of B to A. There are many reasons you might want to do this, including efficiency or cyclical dependencies (i.e. A depends on B depends on A)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9HY2wgKLOr-"
      },
      "source": [
        "# New `nn` Classes\n",
        "\n",
        "You can also create new classes which extend the `nn` module. For these classes, all class attributes, as in `self.layer` or `self.param` will automatically treated as parameters if they are themselves `nn` objects or if they are tensors wrapped in `nn.Parameter` which are initialized with the class.\n",
        "\n",
        "The `__init__` function defines what will happen when the object is created. The first line of the init function of a class, for example, `WellNamedClass`, needs to be `super(WellNamedClass, self).__init__()`.\n",
        "\n",
        "The `forward` function defines what runs if you create that object `model` and pass it a tensor `x`, as in `model(x)`. If you choose the function signature, `(self, x)`, then each call of the forward function, gets two pieces of information: `self`, which is a reference to the object with which you can access all of its parameters, and `x`, which is the current tensor for which you'd like to return `y`.\n",
        "\n",
        "One class might look like the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOip473tQs-d"
      },
      "outputs": [],
      "source": [
        "class ExampleModule(nn.Module):\n",
        "    def __init__(self, input_dims, output_dims):\n",
        "        super(ExampleModule, self).__init__()\n",
        "        self.linear = nn.Linear(input_dims, output_dims)\n",
        "        self.exponent = nn.Parameter(torch.tensor(1.))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear(x)\n",
        "\n",
        "        # This is the notation for element-wise exponentiation,\n",
        "        # which matches python in general\n",
        "        x = x ** self.exponent\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Forward Pass.\n",
        "\n",
        "Forward pass through the network means generating predictions from models.\n",
        "\n",
        "When we pass input data through a neural network in the forward direction to generate outputs, or predictions, the input data flows through the model layers. At each layer, computations performed on the data generate intermediate representations, which are passed to each subsequent layer until the final output is generated. The purpose of the forward pass is to propagate input data through the network and produce predictions or outputs based on the model's learned parameters (weights and biases). This is used for both training and generating new predictions. The final output can be binary classifications, multi-class classifications, or numerical predictions (regressions)."
      ],
      "metadata": {
        "id": "tipdyktNv1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R6zgwWOBv8XF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4CUFH_GS5UY"
      },
      "source": [
        "And you can view its parameters as follows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "id": "YuelIiE4S3KR",
        "outputId": "27a52620-ca40-4dc8-dff5-4f3a56ba0e5b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Parameter containing:\n",
              " tensor(1., requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([[ 0.2789,  0.2618, -0.0678,  0.2766,  0.1436,  0.0917, -0.1669, -0.1887,\n",
              "           0.0913, -0.1998],\n",
              "         [-0.1757,  0.0361,  0.1140,  0.2152, -0.1200,  0.1712,  0.0944, -0.0447,\n",
              "           0.1548,  0.2383]], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([ 0.1881, -0.0834], requires_grad=True)]"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example_model = ExampleModule(10, 2)\n",
        "list(example_model.parameters())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F7E1wKN5tez"
      },
      "source": [
        "And you can print out their names too, as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "id": "dYTuTDsQ5pnY",
        "outputId": "6635a493-7318-4688-bd18-bfba41d43e9d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('exponent',\n",
              "  Parameter containing:\n",
              "  tensor(1., requires_grad=True)),\n",
              " ('linear.weight',\n",
              "  Parameter containing:\n",
              "  tensor([[ 0.2789,  0.2618, -0.0678,  0.2766,  0.1436,  0.0917, -0.1669, -0.1887,\n",
              "            0.0913, -0.1998],\n",
              "          [-0.1757,  0.0361,  0.1140,  0.2152, -0.1200,  0.1712,  0.0944, -0.0447,\n",
              "            0.1548,  0.2383]], requires_grad=True)),\n",
              " ('linear.bias',\n",
              "  Parameter containing:\n",
              "  tensor([ 0.1881, -0.0834], requires_grad=True))]"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(example_model.named_parameters())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWPoIqX2UsaH"
      },
      "source": [
        "And here's an example of the class in action:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "7NXwbg5tUroC",
        "outputId": "0836e447-7c37-464e-b196-048ae0a0cc73"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-0.0567,  0.4562],\n",
              "        [ 0.3780,  0.3452]], grad_fn=<PowBackward1>)"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input = torch.randn(2, 10)\n",
        "example_model(input)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Backword Pass (Back Propagation).\n",
        "Backpropagation is the process by which layer weights and biases are updated during training. All this is part of something called a \"training loop\". This involves propagating data forward, comparing outputs to true values, then propagating backwards to improve each layer's weights and biases using some handy math. We repeat several times until the model is tuned with meaningful weights and biases. In short, during training, the backward pass is the complementary step to the forward pass."
      ],
      "metadata": {
        "id": "Xojf8OnSwZtx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ocol8DABScy"
      },
      "source": [
        "# 2D Operations\n",
        "\n",
        "You won't need these for the first lesson, and the theory behind each of these will be reviewed more in later lectures, but here is a quick reference:\n",
        "\n",
        "\n",
        "*   2D convolutions: [`nn.Conv2d`](https://pytorch.org/docs/master/generated/torch.nn.Conv2d.html) requires the number of input and output channels, as well as the kernel size.\n",
        "*   2D transposed convolutions (aka deconvolutions): [`nn.ConvTranspose2d`](https://pytorch.org/docs/master/generated/torch.nn.ConvTranspose2d.html) also requires the number of input and output channels, as well as the kernel size\n",
        "*   2D batch normalization: [`nn.BatchNorm2d`](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html) requires the number of input dimensions\n",
        "*   Resizing images: [`nn.Upsample`](https://pytorch.org/docs/master/generated/torch.nn.Upsample.html) requires the final size or a scale factor. Alternatively, [`nn.functional.interpolate`](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.interpolate) takes the same arguments.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss Function.\n",
        "We can now assess the differences between actual values and those predicted by the network.\n",
        "\n",
        "2. Why do we need a loss function?\n",
        "We'll do this using a loss function. The loss function tells us how good our model is at making predictions during training. It takes a model prediction, y-hat, and true label, or ground truth, y, as inputs, and outputs a float.\n",
        "\n",
        "Say we are building a model that predicts whether an animal is a mammal (0), bird (1) or other (2). Let's look at a sample from a dataset that contains characteristics about animals. These are the characteristics of a bear, therefore the class is zero (mammal). If our model predicts class equals zero, it is correct and the loss value will be low. If it incorrectly predicts class equals one or two, the loss value will be high. Our goal is to minimize loss."
      ],
      "metadata": {
        "id": "9bsTI-Pxw6KK"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}